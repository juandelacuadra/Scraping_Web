{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== IMPORTACION (EJECUTAR SIEMPRE) ===== #\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "# ME TRAIGO MI CLASE\n",
    "from AxesorMining import AxesorMining\n",
    "\n",
    "\n",
    "print(\"### --- Versiones --- ###\")\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "print(\"### ----------------- ###\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OBTENCION DE TODAS LAS EMPRESAS DE AXESOR _(Con dos cojones)_\n",
    "\n",
    "Ideas generales:\n",
    "\n",
    "- Recorro la carpeta de provincias.\n",
    "- Dentro de cada archivo, recorro los municipios.\n",
    "- Dentro de cada municipio, empieza la fiesta:<br>\n",
    "\n",
    "  En el municipio aparece un listado paginado que parte de 1. En el directorio que generemos insertaremos el número de página para retomar el minado en caso de parada.\n",
    "\n",
    "  En el caso de que solo exista una página, hay que puentear la iteración.\n",
    "\n",
    "  El bucle debe completar todas las empresas de la página antes de pasar a la siguiente.\n",
    "  Debemos leer el total de páginas para saber cuando parar.\n",
    "\n",
    "  Una vez dentro de la página de la empresa, extraemos todos los datos. Cabe la posibilidad de que la empresa esté extinta, por lo que validamos para pasar de largo de ese registro.\n",
    "\n",
    "- SEGURIDAD: Doy un sleep de 3 seg. después de cada REQUEST para evitar el CAPTCHA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DECLARO LAS RUTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PROVINCIAS = 'data/0_Provincias'\n",
    "ROOT_DIRECTORIOS = 'data/1_Directorios'\n",
    "\n",
    "# DEFINO DIRECTORIO POR DEFECTO PARA EL PRIMER BUCLE\n",
    "lista_provincias = os.listdir(ROOT_PROVINCIAS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y AHORA, SUJETAME EL CUBATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_provincia in lista_provincias:\n",
    "\n",
    "    # ========== VARIABLES PARA LA PROVINCIA ========== #\n",
    "\n",
    "    index_localidad = 0\n",
    "    num_pagina = 1\n",
    "    index_pagina = 0\n",
    "\n",
    "    # ========== RUTAS ========== #\n",
    "\n",
    "    # COMPONGO LAS RUTAS DE ESTA ITERACION\n",
    "    path_file = ROOT_PROVINCIAS + '/' + dir_provincia\n",
    "    path_directorio = ROOT_DIRECTORIOS + '/Empresas_' + dir_provincia\n",
    "\n",
    "    print('>> Cargando ' + path_file)\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    # ========== DATAFRAME DE LA PROVINCIA ========== #\n",
    "\n",
    "    # LEO EL CSV DE LA PROVINCIA Y LO METO EN DATAFRAME\n",
    "    df_provincia = pd.read_csv(path_file, sep=\";\")\n",
    "\n",
    "    # EVALUO SI EL DIRECTORIO DE EMPRESAS YA EXISTE Y TIENE ALGUN REGISTRO\n",
    "    if (os.path.exists(path_directorio) and len((pd.read_csv(path_directorio, sep=\";\")).index) != 0):\n",
    "\n",
    "        # SI EXISTE TENGO QUE PESCAR EL ULTIMO VALOR\n",
    "\n",
    "        # OBTENER:\n",
    "        # - index_localidad\n",
    "        # - num_pagina\n",
    "        # - index_pagina\n",
    "\n",
    "        # LEO EL CSV DEL DIRECTORIO DE EMPRESAS Y LO METO EN DATAFRAME\n",
    "        df_directorio = pd.read_csv(path_directorio, sep=\";\")\n",
    "        index_last = df_directorio.index[-1]\n",
    "\n",
    "        # SOBRESCRIBO LAS VARIABLES EN EL PUNTO DONDE LO RETOMO\n",
    "        index_localidad = int(df_directorio.loc[index_last]['Index Localidad'])\n",
    "        num_pagina = int(df_directorio.loc[index_last]['Pagina'])\n",
    "\n",
    "        # EL PRIMER REGISTRO QUE INSERTO\n",
    "        index_pagina = int(df_directorio.loc[index_last]['Index Pagina'] + 1)\n",
    "\n",
    "        # BORRO DE LA PROVINCIA ESOS MUNICIPIOS: YA ESTÁN SACADOS.\n",
    "        df_provincia.drop(\n",
    "            index=df_provincia.index[:index_localidad], axis=0, inplace=True)\n",
    "\n",
    "    else:  # SI NO EXISTE, LO CREO COMO DE COSTUMBRE Y NO ALTERO LAS VARIABLES.\n",
    "\n",
    "        # CREO EL CSV DE LA PROVINCIA VACIO PARA IR AÑADIENDO\n",
    "        df = pd.DataFrame([], columns=AxesorMining().COLUMNAS_DF)\n",
    "        df.to_csv(path_directorio, sep=';', index=False)\n",
    "\n",
    "    # ===================================================== #\n",
    "    # ================= EMPIEZA EL MINADO ================= #\n",
    "    # ===================================================== #\n",
    "\n",
    "    # RECORRO LOS MUNICIPIOS DEL DATAFRAME PROVINCIAL\n",
    "    for index, row in df_provincia.iterrows():\n",
    "\n",
    "        # ========== VARIABLES ========== #\n",
    "        nombre_provincia = row['Provincia']\n",
    "        nombre_municipio = row['Municipio']\n",
    "        url_municipio = row['URL']\n",
    "        url_base = url_municipio[:-1]  # OJO! -> BORRO LA PAGINACION\n",
    "\n",
    "        print('>> Accediendo al directorio de ' + nombre_municipio)\n",
    "\n",
    "        # ========== PAGINADO ========== #\n",
    "        # NUEVO MÉTODO -> OBTENGO CUANTAS PÁGINAS DE RESULTADOS HAY EN ESE MUNICIPIO\n",
    "        page_max = AxesorMining().paginado(url_municipio)\n",
    "\n",
    "        # CREO UN BUCLE CON LAS URL SEGUN PAGINACION.\n",
    "        # EL RANGO VA DE 'NUM_PAGINA' A 'PAGE_MAX' (+1 PARA QUE SEA INCLUIDA)\n",
    "\n",
    "        # ========== RECORRO LAS PÁGINAS ========== #\n",
    "        for page in range(num_pagina, int(page_max)+1):\n",
    "\n",
    "            # EMPIEZO A CRONOMETRAR LA PÁGINA\n",
    "            inicio_pagina = time.time()\n",
    "\n",
    "            # COMPONGO LA URL\n",
    "            print('\\n>> -- Página ', page, ' --')\n",
    "            url_page = url_base + str(page)\n",
    "            print(url_page)\n",
    "\n",
    "            # SACO EL LISTADO DE EMPRESAS DE LA PAGINA\n",
    "            listado_empresas = AxesorMining().listado_empresas(url_page)\n",
    "\n",
    "            # ME CARGO DEL LISTADO LAS EMPRESAS QUE YA EXISTEN\n",
    "            del listado_empresas[0:index_pagina]\n",
    "\n",
    "            # ========== RECORRO LAS EMPRESAS DE LA PÁGINA ========== #\n",
    "            for url_empresa in listado_empresas:\n",
    "\n",
    "                # DECLARO LA LISTA QUE HARÁ DE FILA EN LA TABLA\n",
    "                data_empresas_municipio = []\n",
    "\n",
    "                # EMPAQUETO Y LLAMO AL MÉTODO DE EXTRACCIÓN\n",
    "                url_empresa = 'https:' + url_empresa.get('href').strip()\n",
    "                parametros = [url_empresa, dir_provincia,\n",
    "                              nombre_municipio, page, index_pagina, index_localidad]\n",
    "\n",
    "                try:\n",
    "                    insert_empresa = AxesorMining().obtener_empresa(parametros)\n",
    "                    data_empresas_municipio.append(insert_empresa)\n",
    "                    df = pd.DataFrame(data_empresas_municipio)\n",
    "                    df.to_csv(path_directorio, mode='a',\n",
    "                              sep=';', index=False, header=False)\n",
    "                    print('>> Empresa añadida: ' + insert_empresa['Nombre'])\n",
    "                except:\n",
    "                    print(\"Empresa errónea\")\n",
    "                    continue                # AÑADO AL DATAFRAME E INSERTO EN CSV\n",
    "\n",
    "                # PASO A LA SIGUIENTE POSICIÓN\n",
    "                index_pagina += 1\n",
    "\n",
    "            # PARO DE CRONOMETRAR LA PÁGINA\n",
    "            fin_pagina = time.time()\n",
    "            tiempo_pagina = fin_pagina - inicio_pagina\n",
    "\n",
    "            print('----------------------- FIN DE LA PÁGINA --------------------------')\n",
    "            print('---------------- EXTRAIDA EN ' +\n",
    "                  str(tiempo_pagina) + ' -----------------')\n",
    "\n",
    "            # RESETEO EL INDEX_PAGINA\n",
    "            index_pagina = 0\n",
    "\n",
    "        # SUMO AL INDEX DEL MUNICIPIO\n",
    "        index_localidad += 1\n",
    "        # RESETEO EL NUMERO DE PAGINA\n",
    "        num_pagina = 1\n",
    "\n",
    "        print('>> Municipio finalizado')\n",
    "        print('>> ---')\n",
    "        print(datetime.datetime.now(), '\\n')\n",
    "\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    print('>> --- PROVINCIA FINALIZADA --- ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93e9b880c9b40a6dd17245251b26c21638ffd33fdba627fa2212efe76c291e98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
